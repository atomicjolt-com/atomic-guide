# Story 1.3: AI-Powered Chat Response System

## Status

Draft

## Story

**As a** student using the AI Guide chat interface,
**I want** to receive intelligent, contextually-aware responses powered by AI when I ask questions,
**so that** I can get immediate help that understands my current learning context and provides personalized explanations.

## Acceptance Criteria

1. Chat backend integrates with Cloudflare AI Workers AI for processing messages
2. AI model selection is configurable through admin interface with all available Cloudflare models listed
3. Admin users can set and modify token limits per tenant/course through settings interface
4. System extracts and includes LMS page context (course, module, assignment) in AI prompts
5. AI responses are personalized based on available learner profile data
6. Response time is under 2 seconds for initial token (streaming response)
7. System implements proper error handling for AI service failures with user-friendly fallback messages
8. Token usage is tracked per conversation with admin-configurable limits (10,000 tokens per session default)
9. AI responses support rich media formatting (markdown, LaTeX math, code blocks)
10. System caches frequently asked questions to reduce API calls (FR17)
11. Conversation context is maintained across messages within a session
12. Rate limiting prevents abuse (configurable per tenant, default 10 messages per minute per user)
13. System sanitizes AI responses to prevent XSS and injection attacks
14. Proactive suggestions are generated based on detected patterns (FR15)
15. Model selection persists per tenant and can be changed without code deployment

## Tasks / Subtasks

- [ ] Set up Cloudflare AI Workers AI integration (AC: 1, 2, 15)
  - [ ] Configure AI binding in wrangler.jsonc
  - [ ] Create src/services/AIService.ts with Cloudflare AI client
  - [ ] Implement dynamic model selection from tenant configuration
  - [ ] Create src/services/ModelRegistry.ts to fetch and cache available models
  - [ ] Set up streaming response handler
  - [ ] Add error handling and retry logic with exponential backoff
- [ ] Enhance chat API endpoint with AI processing (AC: 1, 3, 4)
  - [ ] Update src/api/handlers/chat.ts to integrate AIService
  - [ ] Implement prompt template system in src/services/PromptBuilder.ts
  - [ ] Add learner profile context injection
  - [ ] Enable streaming responses via Server-Sent Events (SSE)
  - [ ] Implement response chunking for optimal UX
- [ ] Implement context extraction and enrichment (AC: 2, 3)
  - [ ] Enhance client/hooks/useContentExtractor.ts to capture more context
  - [ ] Create src/services/ContextEnricher.ts for server-side processing
  - [ ] Extract course metadata from LTI claims
  - [ ] Parse page DOM for relevant learning materials
  - [ ] Build context summary for AI prompt inclusion
- [ ] Create FAQ knowledge base system (AC: 8)
  - [ ] Design FAQ schema in src/db/schema.sql (faq_entries table)
  - [ ] Configure Cloudflare Vectorize index in wrangler.jsonc
  - [ ] Create src/services/FAQKnowledgeBase.ts service
  - [ ] Implement similarity matching using Cloudflare Vectorize
  - [ ] Generate embeddings using Cloudflare AI text-embeddings models
  - [ ] Add cache layer using KV namespace for fast retrieval
  - [ ] Create admin endpoints for FAQ management (future story)
- [ ] Implement conversation memory management (AC: 9)
  - [ ] Update ChatConversationDO to maintain conversation history
  - [ ] Implement sliding window context (last 10 messages)
  - [ ] Create conversation summarization for long chats
  - [ ] Add conversation persistence to D1 database
- [ ] Add token tracking and budgeting (AC: 3, 8)
  - [ ] Create token counting utility in src/utils/tokenizer.ts
  - [ ] Implement per-session token tracking in Durable Object
  - [ ] Add token usage to chat response metadata
  - [ ] Create budget enforcement with admin-configurable limits
  - [ ] Store usage metrics in D1 for analytics
  - [ ] Read token limits from tenant configuration
- [ ] Implement rate limiting (AC: 12)
  - [ ] Add rate limiter to ChatConversationDO
  - [ ] Configure default 10 messages/minute limit per user
  - [ ] Allow admin configuration of rate limits per tenant
  - [ ] Return 429 status with retry-after header
  - [ ] Add burst allowance for legitimate rapid questions
- [ ] Add response formatting and sanitization (AC: 7, 11)
  - [ ] Create src/utils/responseFormatter.ts for rich media
  - [ ] Implement markdown parsing with security
  - [ ] Add LaTeX math rendering support markers
  - [ ] Sanitize HTML to prevent XSS attacks
  - [ ] Validate and escape user inputs
- [ ] Implement proactive suggestions (AC: 12)
  - [ ] Create src/services/SuggestionEngine.ts
  - [ ] Detect struggle patterns from conversation
  - [ ] Generate contextual help offers
  - [ ] Add suggestion API endpoint /api/chat/suggestion
  - [ ] Integrate with chat UI for display
- [ ] Create admin configuration interface (AC: 2, 3)
  - [ ] Create client/components/admin/ModelSelector.tsx with dropdown of all models
  - [ ] Add API endpoint GET /api/admin/models to list available Cloudflare models
  - [ ] Create client/components/admin/TokenLimitConfig.tsx for setting limits
  - [ ] Add client/components/admin/RateLimitConfig.tsx for rate configuration
  - [ ] Create admin settings page at /admin/ai-settings route
  - [ ] Implement permission checks for admin access
- [ ] Update client components for AI responses
  - [ ] Enhance client/components/chat/RichMessage.tsx for markdown/LaTeX
  - [ ] Add streaming message display with typing indicator
  - [ ] Implement token usage indicator in chat window
  - [ ] Add retry UI for failed messages
  - [ ] Display proactive suggestions as action cards
  - [ ] Show current AI model in chat header (for admins)
- [ ] Add comprehensive error handling (AC: 5)
  - [ ] Create fallback responses for AI failures
  - [ ] Implement circuit breaker pattern for AI service
  - [ ] Add telemetry for error tracking
  - [ ] Provide helpful error messages to users
  - [ ] Log errors to debug log for analysis
- [ ] Write unit tests for AI integration
  - [ ] Create tests/services/AIService.test.ts
  - [ ] Create tests/services/PromptBuilder.test.ts
  - [ ] Create tests/services/FAQKnowledgeBase.test.ts
  - [ ] Test rate limiting and token tracking
  - [ ] Mock AI responses for consistent testing
  - [ ] Achieve 80% coverage for business logic

## Dev Notes

### Previous Story Insights

Story 1.2 established the complete chat UI infrastructure with React/Redux, chat window components, and basic API endpoints. The ChatConversationDO Durable Object is ready for AI integration. The chat handler at src/api/handlers/chat.ts currently returns placeholder responses and needs AI processing added.

### AI Integration Specifications

[Source: architecture/tech-stack-alignment.md#new-technology-additions]

**Cloudflare AI Configuration:**
- Technology: Cloudflare AI Workers AI
- Purpose: Edge inference with low latency
- Integration: Worker AI binding in wrangler.jsonc
- Model: Configurable per tenant (default: @cf/meta/llama-3.1-8b-instruct)
- Available Models: Dynamically fetched from Cloudflare AI catalog
  - Text Generation: Multiple Llama, Mistral, and other models
  - Code Generation: CodeLlama variants
  - Embeddings: @cf/baai/bge-base-en-v1.5 for generating vectors

**Cloudflare Vectorize Configuration:**
- Purpose: Semantic similarity search for FAQ matching
- Integration: Vectorize binding in wrangler.jsonc
- Index Configuration:
  - Dimensions: 768 (matching BGE model output)
  - Metric: cosine similarity
  - Index name: faq-embeddings

### API Endpoint Specifications

[Source: architecture/api-design-and-integration.md#ai-guide-chat-apis]

**POST /api/chat/message Enhancement:**
```typescript
interface ChatMessageRequest {
  session_id: string;
  message: string;
  page_context: {
    course_id: string;
    module_id?: string;
    page_content?: string;
    current_element?: string;
  };
  conversation_id?: string;
}

interface ChatMessageResponse {
  response: string;
  suggestions?: string[];
  media_attachments?: Array<{
    type: "latex" | "code" | "diagram";
    content: string;
  }>;
  token_usage: {
    used: number;
    remaining: number;
  };
}
```

### Prompt Engineering Guidelines

[Source: architecture/component-architecture.md#5-ai-guide-chat-service]

**System Prompt Template:**
```
You are an AI learning assistant helping a student with {course_name}.
Current context: {module_name} - {assignment_title}
Student profile: {learning_style}, {struggle_areas}
Page content summary: {extracted_content}

Provide a helpful, encouraging response that:
1. Addresses their specific question
2. References the current material
3. Adapts to their learning style
4. Offers additional resources if needed
```

### File Locations

[Source: architecture/source-tree-integration.md]

**New files to create:**
- `src/services/AIService.ts` - Cloudflare AI integration with dynamic model selection
- `src/services/ModelRegistry.ts` - Fetches and caches available Cloudflare models
- `src/services/VectorizeService.ts` - Manages Cloudflare Vectorize operations
- `src/services/EmbeddingService.ts` - Generates embeddings using Cloudflare AI
- `src/services/PromptBuilder.ts` - Prompt template system
- `src/services/ContextEnricher.ts` - Context extraction service
- `src/services/FAQKnowledgeBase.ts` - FAQ system with Vectorize integration
- `src/services/SuggestionEngine.ts` - Proactive help generation
- `src/services/ConfigurationService.ts` - Manages AI config per tenant
- `src/utils/tokenizer.ts` - Token counting utilities
- `src/utils/responseFormatter.ts` - Response formatting/sanitization
- `client/components/chat/RichMessage.tsx` - Rich media message display
- `client/components/admin/ModelSelector.tsx` - Admin model selection UI
- `client/components/admin/TokenLimitConfig.tsx` - Token limit configuration
- `client/components/admin/RateLimitConfig.tsx` - Rate limit configuration
- `client/pages/admin/AISettings.tsx` - Admin settings page
- `src/api/handlers/admin.ts` - Admin API endpoints

**Files to modify:**
- `src/api/handlers/chat.ts` - Add AI processing
- `src/durable_objects/ChatConversationDO.ts` - Add conversation memory
- `client/hooks/useContentExtractor.ts` - Enhanced context extraction
- `client/components/chat/MessageList.tsx` - Streaming display
- `wrangler.jsonc` - Add AI and Vectorize bindings
- `src/db/schema.sql` - Add FAQ and config tables

### Database Schema Additions

[Source: architecture/data-models-and-schema-changes.md]

```sql
-- FAQ Knowledge Base
CREATE TABLE faq_entries (
  id TEXT PRIMARY KEY,
  tenant_id TEXT NOT NULL,
  course_id TEXT,
  question TEXT NOT NULL,
  answer TEXT NOT NULL,
  vector_id TEXT, -- Reference to Cloudflare Vectorize entry
  usage_count INTEGER DEFAULT 0,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_faq_tenant_course ON faq_entries(tenant_id, course_id);
CREATE INDEX idx_faq_vector ON faq_entries(vector_id);

-- Token usage tracking
CREATE TABLE token_usage (
  id TEXT PRIMARY KEY,
  tenant_id TEXT NOT NULL,
  user_id TEXT NOT NULL,
  conversation_id TEXT,
  tokens_used INTEGER NOT NULL,
  model_name TEXT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_token_usage_user ON token_usage(tenant_id, user_id);

-- AI Configuration per tenant
CREATE TABLE ai_config (
  id TEXT PRIMARY KEY,
  tenant_id TEXT NOT NULL UNIQUE,
  model_name TEXT NOT NULL DEFAULT '@cf/meta/llama-3.1-8b-instruct',
  token_limit_per_session INTEGER DEFAULT 10000,
  token_limit_per_day INTEGER DEFAULT 100000,
  rate_limit_per_minute INTEGER DEFAULT 10,
  rate_limit_burst INTEGER DEFAULT 3,
  enabled BOOLEAN DEFAULT TRUE,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_by TEXT -- admin user ID
);

CREATE INDEX idx_ai_config_tenant ON ai_config(tenant_id);
```

### Performance Requirements

[Source: architecture/accessibility-responsive-design-implementation.md#performance-critical-paths]

- Initial response token: <2000ms (streaming)
- Complete response: Progressive rendering
- FAQ cache hit: <100ms response time
- Token counting: <50ms overhead
- Rate limit check: <10ms

### Security Requirements

[Source: architecture/security-integration.md]

- Sanitize all AI responses before display
- Validate token limits before processing
- Rate limit per user to prevent abuse
- Log all AI interactions for audit
- No PII in prompts or logs
- Escape HTML/JavaScript in responses

### Technical Constraints

- Cloudflare Workers CPU limit: 50ms for initial processing
- Streaming responses via Server-Sent Events
- Maximum prompt size: 4096 tokens
- Response streaming chunks: 256 tokens
- Conversation history window: 10 messages
- FAQ cache TTL: 1 hour in KV
- Vectorize index limits: 200,000 vectors per index (free tier)
- Embedding dimensions: 768 (BGE model standard)
- Vector search results: Top 5 similar FAQs returned

## Testing

### Testing Standards

[Source: architecture/testing-strategy.md]

**Test Framework:** Vitest with mock AI responses
**Coverage Targets:** 80% for AI services, 60% for UI

**Required Test Patterns:**
```typescript
// AI Service test example
import { describe, it, expect, vi } from 'vitest';
import { AIService } from '../src/services/AIService';

describe('AIService', () => {
  it('should handle streaming responses', async () => {
    const mockAI = { run: vi.fn().mockResolvedValue({ 
      response: 'Test response' 
    })};
    const service = new AIService(mockAI);
    const result = await service.generateResponse(prompt);
    expect(result).toBeDefined();
  });
});
```

## Change Log

| Date       | Version | Description            | Author             |
| ---------- | ------- | ---------------------- | ------------------ |
| 2025-08-22 | 1.0     | Initial story creation | Bob (Scrum Master) |
| 2025-08-22 | 1.1     | Added configurable models and admin token limits | Bob (Scrum Master) |
| 2025-08-22 | 1.2     | Updated to use Cloudflare Vectorize instead of BLOB storage | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results

### Test Architecture Review - Story 1.3: AI-Powered Chat Response System
**Review Date:** 2025-08-22
**Reviewer:** Quinn (Test Architect)
**Gate Decision:** CONCERNS

### 1. Requirements Traceability Analysis

**Coverage Assessment:**
- ✅ All 12 acceptance criteria have corresponding tasks mapped
- ✅ Clear Given-When-Then scenarios derivable from criteria
- ⚠️ Missing explicit test scenarios for edge cases

**Traceability Matrix:**
| AC | Task Coverage | Test Coverage | Risk Level |
|----|---------------|---------------|------------|
| AC1: AI Integration | Tasks 1,2 | Partial (mocks only) | HIGH |
| AC2: Context Extraction | Task 3 | Not specified | MEDIUM |
| AC3: Personalization | Tasks 2,3 | Not specified | MEDIUM |
| AC4: Response Time <2s | Tasks 1,2 | Performance tests missing | HIGH |
| AC5: Error Handling | Task 11 | Basic coverage | MEDIUM |
| AC6: Token Tracking | Task 5 | Unit tests planned | LOW |
| AC7: Rich Media | Task 8 | Not specified | LOW |
| AC8: FAQ Caching | Task 4 | Unit tests planned | MEDIUM |
| AC9: Conversation Context | Task 5 | Not specified | MEDIUM |
| AC10: Rate Limiting | Task 6 | Unit tests planned | LOW |
| AC11: Security/Sanitization | Task 8 | Critical - not specified | HIGH |
| AC12: Proactive Suggestions | Task 9 | Not specified | LOW |

### 2. Risk Assessment Matrix

**Critical Risks:**
1. **AI Service Reliability (P:High, I:High)** - No integration tests with actual AI service
2. **Performance Under Load (P:Medium, I:High)** - Missing load/stress testing for <2s requirement
3. **Security Vulnerabilities (P:Medium, I:Critical)** - XSS/injection attack prevention not thoroughly tested
4. **Token Budget Overruns (P:Low, I:Medium)** - Edge cases for token limits not covered

**Risk Mitigation Gaps:**
- No chaos engineering tests for AI service failures
- Missing contract tests for AI API integration
- No security penetration testing specified
- Absence of performance regression tests

### 3. Test Design Recommendations

**Critical Test Scenarios Needed:**

```gherkin
# Performance Testing
Scenario: AI Response Under Load
  Given 50 concurrent users sending messages
  When each user sends a complex question
  Then initial token response time < 2000ms for 95th percentile
  And complete response streams successfully
  And no memory leaks in Durable Objects

# Security Testing
Scenario: XSS Attack Prevention
  Given a malicious prompt containing <script> tags
  When AI responds with HTML-like content
  Then response is properly sanitized
  And no scripts execute in client

# Resilience Testing
Scenario: AI Service Degradation
  Given AI service responding slowly (>5s)
  When user sends a message
  Then circuit breaker activates after 3 failures
  And fallback response is provided
  And user sees helpful error message
```

**Test Coverage Gaps:**
- Integration tests with real Cloudflare AI
- End-to-end streaming response tests
- Conversation context persistence tests
- FAQ similarity matching accuracy tests
- Rate limiting boundary tests
- Token counting accuracy validation

### 4. Non-Functional Requirements Assessment

**Performance:**
- ⚠️ No load testing strategy defined
- ⚠️ Missing latency budgets per component
- ❌ No performance baseline established

**Security:**
- ⚠️ Sanitization testing not comprehensive
- ❌ No OWASP Top 10 validation mentioned
- ❌ Missing security audit requirements

**Reliability:**
- ✅ Circuit breaker pattern planned
- ⚠️ No SLA targets defined
- ❌ Missing disaster recovery tests

**Scalability:**
- ⚠️ Durable Object limits not tested
- ❌ No capacity planning tests

### 5. Testability Analysis

**Controllability:** MEDIUM
- ✅ AI responses can be mocked
- ⚠️ Difficult to control AI model behavior
- ❌ No test data generation strategy

**Observability:** LOW
- ⚠️ Limited logging mentioned
- ❌ No distributed tracing specified
- ❌ Missing test metrics collection

**Determinism:** LOW
- ❌ AI responses inherently non-deterministic
- ⚠️ No golden dataset for validation
- ❌ Missing regression test baselines

### 6. Technical Debt Identified

1. **Test Infrastructure Debt:**
   - No AI response validation framework
   - Missing performance test harness
   - Lack of security testing tools

2. **Coverage Debt:**
   - Integration test coverage < 40%
   - No contract tests
   - Missing negative test scenarios

3. **Documentation Debt:**
   - No test plan document
   - Missing test data specifications
   - Undefined acceptance test criteria

### 7. Recommended Actions

**Must Fix Before Release:**
1. Add comprehensive security testing for XSS/injection
2. Implement performance testing suite for <2s requirement
3. Create integration tests with AI service (using test account)
4. Add explicit error scenario testing

**Should Address Soon:**
1. Implement contract tests for AI API
2. Add conversation context persistence tests
3. Create FAQ similarity matching validation
4. Define and test rate limiting boundaries

**Nice to Have:**
1. Chaos engineering tests
2. Load testing with realistic conversation patterns
3. A/B testing framework for prompt optimization
4. Automated security scanning integration

### 8. Test Implementation Priority

**Phase 1 - Critical Path (Sprint 1):**
- Security sanitization tests
- Basic performance benchmarks
- AI service integration tests (with mocks)
- Error handling validation

**Phase 2 - Core Functionality (Sprint 2):**
- Streaming response tests
- Token tracking accuracy
- Rate limiting tests
- FAQ caching validation

**Phase 3 - Quality Enhancement (Sprint 3):**
- Load testing suite
- Conversation context tests
- Proactive suggestion validation
- End-to-end user journey tests

### 9. Quality Metrics Targets

| Metric | Target | Current | Gap |
|--------|--------|---------|-----|
| Unit Test Coverage | 80% | Planned | - |
| Integration Coverage | 60% | 0% | 60% |
| Performance Tests | 10 scenarios | 0 | 10 |
| Security Tests | 15 cases | 0 | 15 |
| API Contract Tests | 100% | 0% | 100% |
| E2E Happy Paths | 5 flows | 0 | 5 |
| Error Scenarios | 20 cases | Partial | ~15 |

### 10. Gate Decision Rationale

**Decision: CONCERNS**

**Reasoning:**
- Story is well-structured with clear requirements
- Implementation approach is sound
- However, critical test gaps exist in security, performance, and integration
- AI service reliability testing is insufficient for production readiness

**Conditions for PASS:**
1. Add explicit security test scenarios
2. Define performance testing strategy
3. Include integration test approach with AI service
4. Specify error scenario coverage
5. Add observability/monitoring test requirements

**Recommendations:**
- Allocate 30% of sprint capacity to testing activities
- Consider dedicated security testing sprint
- Implement progressive rollout with feature flags
- Establish performance baselines before full release